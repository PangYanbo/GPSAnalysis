{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1aa5e1f-5e70-4cad-b253-7435f536971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da8fc5-a68e-4a29-aa3c-6c51ee691866",
   "metadata": {},
   "source": [
    "# 创建一个SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aec94a3-b08c-45bc-b158-ddaa1dbcc87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/20 15:59:40 WARN Utils: Your hostname, owner-Super-Server resolves to a loopback address: 127.0.1.1; using 157.82.142.123 instead (on interface eno1)\n",
      "24/08/20 15:59:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/20 15:59:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Split Large CSV by Date\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1a537-20f2-458b-8f36-33a9b6ac0c44",
   "metadata": {},
   "source": [
    "# 读取CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8375eea8-d7d8-4c86-b382-ec04b69d6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/mnt/large/data/BlogWatcher/Tokyo_202307_202308/BW_Tokyo_202307_202308.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d050f4-616e-4d80-a77d-551b4abf8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 将datetime列转换为日期格式\n",
    "df = df.withColumn('date', to_date(col('datetime')))\n",
    "\n",
    "# 获取所有唯一日期\n",
    "dates = df.select('date').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e598430-b1d5-403c-91dc-6283c46bd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出文件夹路径\n",
    "output_folder = 'output_files'\n",
    "\n",
    "# 按日期筛选数据并保存为单独的文件\n",
    "for row in dates:\n",
    "    print(row)\n",
    "    break\n",
    "    date = row['date'].strftime('%Y-%m-%d')\n",
    "    filtered_df = df.filter(col('date') == date)\n",
    "    output_file = f\"{output_folder}/{date}.csv\"\n",
    "    filtered_df.write.csv(output_file, header=True, mode='overwrite')\n",
    "\n",
    "print(\"数据切分完成！\")\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334780b9-ecd7-4104-88ab-15d2f5e58d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
