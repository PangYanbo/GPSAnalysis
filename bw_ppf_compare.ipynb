{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67155f7e-f207-4bff-b753-39973ff55cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1855bd9-8f03-47e1-ae6a-f4a1b076dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to calculate r_g, random_entropy, and waiting_times\n",
    "def calculate_metrics(df_chunk):\n",
    "    # Calculate centroid and r_g\n",
    "    centroid_lon = df_chunk['lon'].mean()\n",
    "    centroid_lat = df_chunk['lat'].mean()\n",
    "    distances = np.sqrt((df_chunk['lon'] - centroid_lon)**2 + (df_chunk['lat'] - centroid_lat)**2)\n",
    "    rg = np.sqrt((distances**2).mean())\n",
    "    \n",
    "    # Calculate random entropy (based on the distribution of locations)\n",
    "    position_counts = df_chunk.groupby(['lon', 'lat']).size()\n",
    "    probabilities = position_counts / position_counts.sum()\n",
    "    random_entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    # Calculate waiting times\n",
    "    df_chunk = df_chunk.sort_values(by='timestamp')\n",
    "    df_chunk['time_diff'] = df_chunk['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "    waiting_times = df_chunk.groupby(['lon', 'lat'])['time_diff'].sum().mean()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'rg': rg,\n",
    "        'random_entropy': random_entropy,\n",
    "        'waiting_times': waiting_times\n",
    "    })\n",
    "\n",
    "# 2. Function to fetch data from the database and calculate r_g based on different time periods\n",
    "def fetch_data_by_time_period(time_period):\n",
    "    # Construct the query based on the time period\n",
    "    if time_period == 'week':\n",
    "        date_trunc = \"DATE_TRUNC('week', gps_logs.logtime)\"\n",
    "    elif time_period == '2weeks':\n",
    "        date_trunc = \"FLOOR(EXTRACT('epoch' FROM gps_logs.logtime) / (2 * 7 * 24 * 60 * 60)) * (2 * 7 * 24 * 60 * 60)\"\n",
    "    elif time_period == '3weeks':\n",
    "        date_trunc = \"FLOOR(EXTRACT('epoch' FROM gps_logs.logtime) / (3 * 7 * 24 * 60 * 60)) * (3 * 7 * 24 * 60 * 60)\"\n",
    "    elif time_period == 'month':\n",
    "        date_trunc = \"DATE_TRUNC('month', gps_logs.logtime)\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported time period\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH centroid_per_user AS (\n",
    "        SELECT \n",
    "            uuid, \n",
    "            ST_Centroid(ST_Collect(geom)) AS centroid_geom,\n",
    "            {date_trunc} AS time_period\n",
    "        FROM blogwatcher.gps_logs gps_logs\n",
    "        WHERE gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n",
    "        GROUP BY uuid, time_period\n",
    "    )\n",
    "    SELECT \n",
    "        gps_logs.uuid, \n",
    "        {date_trunc} AS time_period,\n",
    "        SQRT(SUM(POW(ST_Distance(gps_logs.geom, c.centroid_geom), 2)) / COUNT(*)) AS radius_of_gyration\n",
    "    FROM \n",
    "        blogwatcher.gps_logs gps_logs\n",
    "    JOIN \n",
    "        centroid_per_user c \n",
    "    ON \n",
    "        gps_logs.uuid = c.uuid AND {date_trunc} = c.time_period\n",
    "    WHERE \n",
    "        gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n",
    "    GROUP BY \n",
    "        gps_logs.uuid, time_period\n",
    "    ORDER BY \n",
    "        time_period;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and return the results\n",
    "    conn = psycopg2.connect(\n",
    "        dbname = \"gisdb\", # \"bw_tokyo/bw_ishikawa\" \n",
    "        user = \"postgres\",\n",
    "        password =  \"task4TH\",\n",
    "        host = \"localhost\",\n",
    "        port = '5432', \n",
    "    )\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Visualization function\n",
    "def compare_and_visualize(df_metrics_gps, df_metrics_pflow):\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    # 1. Comparison of r_g probability distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for label, df in zip(['GPS', 'Pseudo PFLOW'], [df_metrics_gps, df_metrics_pflow]):\n",
    "        rg_values = df['rg'].dropna().values\n",
    "        hist, bin_edges = np.histogram(rg_values, bins=np.logspace(np.log10(1), np.log10(1000), 50), density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        plt.loglog(bin_centers, hist, 'o-', label=label)\n",
    "    plt.xlabel(r'$r_g$ (km)')\n",
    "    plt.ylabel(r'$P(r_g)$')\n",
    "    plt.title('Comparison of r_g Probability Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    \n",
    "    # 2. Comparison of random_entropy density plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for label, df in zip(['GPS', 'Pseudo PFLOW'], [df_metrics_gps, df_metrics_pflow]):\n",
    "        df['random_entropy'].plot(kind='density', label=label)\n",
    "    plt.xlabel('Entropy')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Comparison of Random Entropy Density Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. Comparison of waiting_times histogram\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for label, df in zip(['GPS', 'Pseudo PFLOW'], [df_metrics_gps, df_metrics_pflow]):\n",
    "        plt.hist(df['waiting_times'].dropna().values, bins=50, alpha=0.7, label=label)\n",
    "    plt.xlabel('Waiting Time (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Comparison of Waiting Times Histogram')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4836b40-5104-4ea8-a41c-fffcdb01c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3538583/3809587830.py:75: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n    WITH centroid_per_user AS (\n        SELECT \n            uuid, \n            ST_Centroid(ST_Collect(geom)) AS centroid_geom,\n            DATE_TRUNC('week', gps_logs.logtime) AS time_period\n        FROM blogwatcher.gps_logs gps_logs\n        WHERE gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n        GROUP BY uuid, time_period\n    )\n    SELECT \n        gps_logs.uuid, \n        DATE_TRUNC('week', gps_logs.logtime) AS time_period,\n        SQRT(SUM(POW(ST_Distance(gps_logs.geom, c.centroid_geom), 2)) / COUNT(*)) AS radius_of_gyration\n    FROM \n        blogwatcher.gps_logs gps_logs\n    JOIN \n        centroid_per_user c \n    ON \n        gps_logs.uuid = c.uuid AND DATE_TRUNC('week', gps_logs.logtime) = c.time_period\n    WHERE \n        gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n    GROUP BY \n        gps_logs.uuid, time_period\n    ORDER BY \n        time_period;\n    ': column \"gps_logs.logtime\" must appear in the GROUP BY clause or be used in an aggregate function\nLINE 13:         DATE_TRUNC('week', gps_logs.logtime) AS time_period,\n                                    ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGroupingError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2018\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mGroupingError\u001b[0m: column \"gps_logs.logtime\" must appear in the GROUP BY clause or be used in an aggregate function\nLINE 13:         DATE_TRUNC('week', gps_logs.logtime) AS time_period,\n                                    ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load and analyze GPS data for different time periods\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_metrics_gps_week \u001b[38;5;241m=\u001b[39m fetch_data_by_time_period(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_metrics_gps_week\n",
      "Cell \u001b[0;32mIn[40], line 75\u001b[0m, in \u001b[0;36mfetch_data_by_time_period\u001b[0;34m(time_period)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Execute the query and return the results\u001b[39;00m\n\u001b[1;32m     68\u001b[0m conn \u001b[38;5;241m=\u001b[39m psycopg2\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m     69\u001b[0m     dbname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgisdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# \"bw_tokyo/bw_ishikawa\" \u001b[39;00m\n\u001b[1;32m     70\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     port \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5432\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     74\u001b[0m )\n\u001b[0;32m---> 75\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query, conn)\n\u001b[1;32m     76\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/sql.py:564\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    561\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[1;32m    565\u001b[0m         sql,\n\u001b[1;32m    566\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m    567\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    568\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m    569\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m    570\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    571\u001b[0m     )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/sql.py:2078\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2068\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     dtype: DtypeArg \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2075\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m   2077\u001b[0m     args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2078\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   2079\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/sql.py:2030\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2030\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\n    WITH centroid_per_user AS (\n        SELECT \n            uuid, \n            ST_Centroid(ST_Collect(geom)) AS centroid_geom,\n            DATE_TRUNC('week', gps_logs.logtime) AS time_period\n        FROM blogwatcher.gps_logs gps_logs\n        WHERE gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n        GROUP BY uuid, time_period\n    )\n    SELECT \n        gps_logs.uuid, \n        DATE_TRUNC('week', gps_logs.logtime) AS time_period,\n        SQRT(SUM(POW(ST_Distance(gps_logs.geom, c.centroid_geom), 2)) / COUNT(*)) AS radius_of_gyration\n    FROM \n        blogwatcher.gps_logs gps_logs\n    JOIN \n        centroid_per_user c \n    ON \n        gps_logs.uuid = c.uuid AND DATE_TRUNC('week', gps_logs.logtime) = c.time_period\n    WHERE \n        gps_logs.logtime BETWEEN '2023-07-22' AND '2023-08-22'\n    GROUP BY \n        gps_logs.uuid, time_period\n    ORDER BY \n        time_period;\n    ': column \"gps_logs.logtime\" must appear in the GROUP BY clause or be used in an aggregate function\nLINE 13:         DATE_TRUNC('week', gps_logs.logtime) AS time_period,\n                                    ^\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze GPS data for different time periods\n",
    "df_metrics_gps_week = fetch_data_by_time_period('week')\n",
    "df_metrics_gps_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ca57e-17e7-4874-b7b5-b402e8b4c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_gps_2weeks = fetch_data_by_time_period('2weeks')\n",
    "df_metrics_gps_2weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69cbe4-673d-489a-8e19-2477f4ec9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_gps_3weeks = fetch_data_by_time_period('3weeks')\n",
    "df_metrics_gps_3weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d108f4-541a-456e-90a9-22b56cfc18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_gps_month = fetch_data_by_time_period('month')\n",
    "df_metrics_gps_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b78bb-3d91-41e3-aa15-fcdde2dcdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze Pseudo PFLOW data\n",
    "df_metrics_pflow = analyze_pseudo_pflow('path_to_pseudo_pflow.csv')\n",
    "df_metrics_pflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87950cfd-05f6-4913-a244-9e95bb360baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and visualize the results\n",
    "compare_and_visualize(df_metrics_gps_week, df_metrics_pflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
